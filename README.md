# MapReduceWebScraper

Для запуска скрапера достаточно выполнить в командной строке
```commandline
bash WebScraperMainScript depth 
```
где depth - глубина захода.

Формат данных:
``Key\tValue\n``
где Key - ссылка длиной не более MAX_LINK_LENGTH
(константы можно изменять в constants.h), а Value = 1, если мы 
еще не обрабатывали ссылку, и 0 иначе.

Скрипт запускает:

1) "map". В качестве map_script передается scraper.py, 
который выполняется питоном. "Map" делит файл на файлики, содержащие 
LENGTH_TO_DIVIDE_MAP строк, и запускает скрипт
 на каждом из них. Потом мерджит выводы в один temp.txt
 
1) "reduce".
   
   Сортировка. Используется сортировка бинарных файлов, поэтому перед и после 
   сорта идет конвертация из/в текстовый формат. 
   
   @TODO write normal mergesort with usual strings
   
   В качестве скрипта передается ExtUnique.cpp, который, 
   храня только текущую и предыдущую строки, оставляет уникальные значения. Скрипт 
   также учитывает особенности ссылок, например, следующие ссылки считаются за одну:
     - https://ya.ru, 
     - https://ya.ru/.    
   
   Также стоит отменить, что если у нас есть несколько строк с одними ключами и 
   разными значениями(0 - посещено, 1 - не посещено), то останется только строка с 0
   (так как сортим по возрастанию и она будет раньше)
   
   #### Запуск скрипта
  